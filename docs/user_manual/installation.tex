\section{Installation}

\setcounter{equation}{0}

It should be possible to build and run PFLOTRAN on essentially any system with 
modern C and Fortran (2003 standard or later) compilers and an available 
implementation of the Message Passing Interface (MPI) system that has been 
built with Fortran bindings.
Besides these requirements, the major third-party library required is 
the open-source library PETSc---the Portable, Extensible Toolkit for Scientific 
Computation---that provides the parallel framework on which PFLOTRAN is 
built.
Most of the work involved in building PFLOTRAN lies in building PETSc. 
PETSc uses a sophisticated Python-based build tool, BuildSystem, to perform 
extensive platform discovery and configuration as well as automatic download 
and build for any of the open-source third-party libraries that PETSc can 
use. 
The PFLOTRAN makefiles use the information generated by BuildSystem as part 
of the PETSc build process; once PETSc is built, building PFLOTRAN is  
straightforward.

Besides MPI and PETSc, libraries commonly installed are

\begin{itemize}
\item HDF5, required for parallel I/O and reading HDF5-formatted input files
\item ParMetis and Metis, graph partitioning libraries required for 
      unstructured grids
\item Hypre, which provides a variety of preconditioners and multilevel 
      solves
\end{itemize}

\subsection{Compilers}

The installation of PFLOTRAN on MacOSX requires compiler versions 4.7 or later for gfortran and gcc to be compatible with Fortran 2003. 

\begin{comment}
\subsection{MacOSX}

The installation of PFLOTRAN on MacOSX requires compiler versions 4.7.2 or later for gfortran and gcc and must be compatible with object oriented features in Fortran 2003. 
%The compiler g++ is needed for installing SAMRAI.
%Define environment variables: {\tt MPI\_HOME}, {\tt PKGS}, \linebreak {\tt HYPRE\_INSTALL\_DIR}

\subsubsection{OpenMPI}

\begin{verbatim}
setenv F90 gfortran
setenv F77 gfortran
setenv FC gfortran
setenv CC gcc-4
setenv CXX g++-4

./configure \
--prefix=$PKGS/openmpi/openmpi-1.4.1-gcc-4.4.2-gfortran \
--disable-debug \
--enable-static \
--disable-shared

make
make install
\end{verbatim}

\subsubsection{Hypre}

\begin{verbatim}
./configure \
--with-MPI \
--enable-debug \
--disable-opt \
--prefix=${HYPRE_INSTALL_DIR} \
--with-MPI-include=${MPI_HOME}/include \
--with-MPI-libs=openmpi \
--with-MPI-lib-dirs=${MPI_HOME}/lib \
CC=mpicc \
CXX=mpicxx \
F77=mpif90

make
make install
\end{verbatim}
\end{comment}

\subsection{Building PETSc}

The first step to building PFLOTRAN is to configure and build the PETSc 
toolkit.
This requires, at minimum, working installations of C and Fortran 95-compliant 
compilers.
For users looking for an open-source compiler, we recommend the gcc and 
gfortran compilers that are part of the GNU Compiler Collection (GCC), version 
4.4 or later.
Users may also wish to install MPI and other libraries from source or via a 
package manager, but the PETSc {\tt ./configure} script can be used not only 
to install PETSc but also MPI, HDF5, ParMETIS/METIS, and various solver 
libraries such as MUMPS for sparse direct solvers, Hypre for a variety of 
preconditioners and multi-level solvers, and the Trilinos multilevel solver ML. 
For systems that do not provide specially optimized versions of these 
libraries, we recommend using PETSc's {\tt configure} to install these 
third-party libraries.
If you do wish to install any of these third-party libraries yourself, you 
will need to do so {\it before} installing PETSc to that the necessary PETSc 
interfaces to these packages can be built.

The development branch of PFLOTRAN tracks the main development branch of 
PETSc and hence requires this ``petsc-dev'' version, which can be either 
downloaded from the PETSc web page 
\href{http://www.mcs.anl.gov/petsc/developers/index.html}{petsc-dev}, 
or installed using Mercurial following instructions on the PETSc developer web 
page. 
We recommend that PETSc be obtained using the version control system  
(\href{http://mercurial.selenic.com/wiki}{Mercurial}).

Define environment variables $\verb|PETSC_DIR|$ and $\verb|PETSC_ARCH|$ giving the location of the petsc-dev source and the directory where the libraries for the particular architecture are stored after compiling and installing PETSc.

\subsubsection{MacOSX}

To install ParMETIS/METIS on MacOSX it is necessary to first install CMAKE from e.g. MacPorts or Fink.

To install PETSc, MPI using openmpi, HDF5, and ParMETIS/METIS with debugging turned off configure PETSc using:

\begin{verbatim}
./config/configure.py --with-debugging=0 --with-shared-libraries=0 
--with-x=0 --download-openmpi --download-hdf5=1 
--download-metis=1 --download-parmetis=1
\end{verbatim}

\noindent
followed by

$\verb|make all|$

\noindent
and

$\verb|make test|$

Note that ParMETIS/METIS is only needed for using the unstructured grid capability in PFLOTRAN. HDF5 is recommended for large problems for use with Visit for (parallel) visualization.

\subsubsection{Windows}

To install PETSc and PFLOTRAN on Windows see instructions on the PFLOTRAN wiki \linebreak ({\tt https://bitbucket.org/pflotran/pflotran-dev/wiki/Home}).

\begin{comment}
\subsubsection{MacOSX}

\begin{verbatim}
./config/configure.py \
--with-mpi-dir=$MPI_HOME \
--download-openmpi=1 \
--with-debugging=0 \
--with-shared-libraries=0 \
--download-mumps=1 \
--download-parmetis=1 \
--download-scalapack=1 \
--download-blacs=1 \
--download-ml=1 \
--download-hdf5=1 

make all
make test
\end{verbatim}

\begin{enumerate}
\item Obtain C compiler gcc4.5 or later for snow leopard (e.g. from http://hpc.sourceforge.net). 
%They just had gcc4.6 available recently. 
Install gcc under \verb|/opt/local| or \verb|/usr/local|. 
You don't have to get gcc4.5 or 4.6,
but you have to have a verison not older than gcc4.3 for some features in
pflotran. Both gcc4.5 and gcc4.6 come with gfortran.

\item Install openmpi 1.5.1 from www.open-mpi.org. Configure using the command:

\begin{verbatim}
./configure CC=gcc CXX=g++ FC=gfortran --enable-mpi-f77=no 
--prefix=/opt/local
\end{verbatim}

\item Download petsc-dev and configure using

\begin{verbatim}
./configure --download-hdf5=1 --with-cc=mpicc --with-cxx=mpicxx 
--with-fc=mpif90,
\end{verbatim}

then compile with \verb|make all|.

\item Compile pflotran using

\verb|make pflotran|
\end{enumerate}
\end{comment}

\subsubsection{ORNL's Jaguar XT4/5}

\footnotesize
\begin{verbatim}
./config/configure.py PETSC_ARCH=cray-xt4-pgi_fast \
--configModules=PETSc.Configure \
--optionsModule=PETSc.compilerOptions \
--known-level1-dcache-size=65536 \
--known-level1-dcache-linesize=64 \
--known-level1-dcache-assoc=2 \
--known-memcmp-ok=1 \
--known-sizeof-char=1 \
--known-sizeof-void-p=8 \
--known-sizeof-short=2 \
--known-sizeof-int=4
--known-sizeof-long=8 \
--known-sizeof-long-long=8 \
--known-sizeof-float=4 \
--known-sizeof-double=8 \
--known-sizeof-size_t=8 \
--known-bits-per-byte=8 \
--known-sizeof-MPI_Comm=4 \
--known-sizeof-MPI_Fint=4 \
--known-mpi-long-double=0 \
--with-batch=1 \
--with-shared-libraries=0 \
--with-dynamic=0 \
--with-cc=cc \
--with-cxx=CC \
--with-fc=ftn \
--COPTFLAGS="-tp barcelona-64 -fastsse -Mipa=fast" \
--CXXOPTFLAGS="-tp barcelona-64 -fastsse -Mipa=fast" \
--FOPTFLAGS="-tp barcelona-64 -fastsse" \
--with-debugging=0 \
--with-blas-lib=sci \
--with-lapack-lib=sci \
--with-x=0 \
--with-mpi-dir=$MPICH_DIR \
--download-hypre=1 \
--download-parmetis=1 \
--with-hdf5=1 \
--with-hdf5-dir=$HDF5_DIR \
--known-mpi-shared=0
\end{verbatim}
\normalsize
The user will need to load the following HDF5 module beforehand: \ \verb|module load hdf5-parallel|.

\subsection{Building PFLOTRAN}

The source code for PFLOTRAN can be downloaded from the web site: \verb|hg clone https://bitbucket.org/pflotran/pflotran-dev| using Mercurial. 
PFLOTRAN is compiled with the command:

\verb|make [options] pflotran|

\noindent
where several possible options are: 

\begin{tabular}{ll}
\verb|scco2=1|: & --enable two-phase supercritical CO$_2$ mode\\
%\verb|hdf5|: & --enable HDF5 I/O capability\\
\verb|coll=1|: & --enable colloid-facilitated transport option\\
%\verb|temp|: & --enable temperature dependent log $K$ capability\\
\verb|mfd=1|: & --enable MFD full permeability tensor capability (not operational)
%\verb|ice|:& --enable permafrost hydrology (works only in THC mode)
\end{tabular}

\noindent
Thus for compiling with the supercritical CO$_2$ option use: {\tt make ssco2=1 pflotran}.
See the PFLOTRAN \verb|makefile| for additional options.

\subsection{Parallel I/O using Sarat's PARALLELIO\_LIB with PFLOTRAN}

The parallel I/O library enables a scalable general purpose parallel I/O capability for HPC by taking advantage of existing parallel I/O libraries, such as HDF5 which are being widely used by scientific applications, and modifing these algorithms to better scale on larger number of processors.

Values for the {\tt HDF5\_READ\_GROUP\_SIZE} and {\tt HDF5\_WRITE\_GROUP\_SIZE} options must be set in the input file. 
Typical values are to set the
write group size equal to the number of processes on a compute node (typically 16 or 32). If that still results in a penalty, decrease it further. 
A much higher read group size is preferred, e.g. use 512 when running on 512 cores and then one process reads the input file and broadcasts relevant sections.
Put {\tt HDF5\_WRITE\_GROUP\_SIZE} under the {\tt OUTPUT} keyword:
\footnotesize
\begin{Verbatim}
:=========================== output options ===================================
OUTPUT
  TIMES y 5. 10. 15. 20.
  FORMAT TECPLOT POINT
  HDF5_WRITE_GROUP_SIZE 16
END
\end{Verbatim}
\normalsize
and {\tt HDF5\_READ\_GROUP\_SIZE} in the main body of the input file:
\footnotesize
\begin{Verbatim}
HDF5_READ_GROUP_SIZE 1024
\end{Verbatim}
\normalsize
Number of MPI tasks should be an exact multiple of {\tt HDF\_READ\_GROUP\_SIZE}.

For more details on the I/O library, please see Appendix A (Figs. A4, A5) in Sarat Sreepathi <admin@sarats.com> \href{http://www.lib.ncsu.edu/resolver/1840.16/8317}{dissertation}.

Instructions for downloading and installing the parallel I/O library for use with PFLOTRAN is provided below. Note that this software is separate from PFLOTRAN and under a LGPL.
\begin{enumerate}

\item Download parallelio\_lib source code for PARALLELIO library:

{\small\tt svn co http://ascem-io.secure-water.org/ascem-io/trunk DIRNAME}

where {\tt DIRNAME} is the installation directory. 
%The username and password are:
%\href{https://bitbucket.org/pflotran/pflotran-dev/wiki/Documentation/Strata}{Strata}

%username: {\tt pflotran\_dev}

%password: {\tt gr0undw@t3r}

\item Compile PARALLELIO library:

To compile the parallel library first modify the Makefile by adding the lines following the {\tt endif} statement:
\footnotesize
\begin{Verbatim}
ifeq ($(MACHINE),jaguarpf)
	CC=cc
	FC=ftn
	LINKER=ftn
	CFLAGS+= -O3 
	FFLAGS+= -O3 
	LDFLAGS+= -lhdf5 -lz
endif

CC=${PETSC_DIR}/${PETSC_ARCH}/bin/mpicc
FC=${PETSC_DIR}/${PETSC_ARCH}/bin/mpif90
LINKER=${FC}
CFLAGS+= -I${PETSC_DIR}/${PETSC_ARCH}/include -I../../src -O3 
FFLAGS+= -O3 
LDFLAGS+= -L../../src -lparallelio -Wl,-L${PETSC_DIR}/${PETSC_ARCH}/lib -lhdf5 -lz 
\end{Verbatim}

{\tt cd DIRNAME/src}

{\tt make} \hfill {\it (compile with {\tt mpicc})}
\normalsize

\item Compile PFLOTRAN:

Define environmental variable: {\tt PARALLELIO\_LIB} giving path to PARALLELIO library:

\footnotesize
{\tt export PARALLELIO\_LIB=\$PWD} \hfill {\it (bash shell)}
\normalsize

or

\footnotesize
{\tt setenv PARALLELIO\_LIB \$PWD} \hfill {\it (tcsh/csh shell)}

{\tt cd PFLOTRAN\_DIR/src/pflotran}

{\tt make have\_parallelio\_lib=1 pflotran}
\normalsize
\end{enumerate}

\scriptsize
\begin{verbatim}
README
-------------------------------------------------------------------------------
ASCEM-IO
Scalable Parallel I/O module for Environmental Management Applications
-------------------------------------------------------------------------------
This library provides software that read/write data sets from/to parallel file 
systems in an efficient and scalable manner. 
In this context, scalable means that the simulators read/write 
performance does not degrade significantly as the number of cores grows.

-------------------------------------------------------------------------------
COPYRIGHT AND LICENSE 
-------------------------------------------------------------------------------
ASCEM-IO is distrubuted under the terms of the GNU Lesser General Public 
License (LGPL). The copyright is held jointly by North Carolina State University 
and Pacific Northwest National Laboratory. 

The copyright and license information is specified in the included file 
COPYRIGHT. 

-------------------------------------------------------------------------------
Repository Access
-------------------------------------------------------------------------------
Please request write access by contacting Kumar Mahinthakumar (gmkumar@ncsu.edu). 
Use the following command to access repository:
	svn co http://ascem-io.secure-water.org/ascem-io

-------------------------------------------------------------------------------
Building Library
-------------------------------------------------------------------------------
The current stable release is 2.2.

--------------
Prerequisites:
--------------
MPI
C compiler
HDF5 libraries (preferably with parallel(MPI) support)
Optional: Fortran (for Fortran example)

After downloading ASCEM-IO and gathering details of HDF5 installation, 
the following commands can be used to build and install ASCEM-IO: 

	cd <ASCEM-IO check out directory>/src
	make CC=<C-compiler> HDF5_INCLUDE_DIR=<location of the HDF5 include directory>
	make ASCEMIO_INSTALL_DIR=<user defined install location> install

In this case, CC refers to C compiler with MPI support, e.g., mpicc.

-------------------------------------------------------------------------------
\end{verbatim}

\normalsize

\section{Running PFLOTRAN}

PFLOTRAN can be run from the command line as

\verb|mpirun -np 10 pflotran [options]|

A number of command line options are available:

\begin{tabular}{ll}
\toprule
\multicolumn{2}{c}{PFLOTRAN}\\
\midrule
{\tt -pflotranin} <input file name> & specify input file [Default: {\tt pflotran.in}]\\
{\tt -file\_prefix} <output file name> & specify output file [Default: {\tt pflotran.out}]\\
{\tt -screen\_output off} & turn off screen output\\
%{\tt -v} &\\
{\tt -realization\_id <integer>} & run specified realization ID\\
{\tt -multisimulation} & run multiple input files in one run\\
{\tt -stochastic} & Monte Carlo multiple realization run\\
\midrule
\multicolumn{2}{c}{PETSc}\\
\midrule
{\tt -log\_summary} & print out run performance (PETSc option)\\
{\tt -on\_error\_abort} & aborts run on hitting NaNs\\
\bottomrule
\end{tabular}

\subsection*{Updating PFLOTRAN}

To update the PFLOTRAN source code
type: {\tt hg pull -u} from within the PFLOTRAN source repository.

\noindent
Recompile PFLOTRAN using:\\
\indent
{\tt make clean}\\
\indent
{\tt make [options] pflotran}

\subsection*{Multiple Realization Simulation Mode}

To launch 1000 realizations on 100 processor groups using 10,000 processor cores:

\noindent
{\tt mpirun -np 10000 pflotran -stochastic -num\_realizations 1000 

-num\_groups 100}

Each processor group will utilize 100 processor cores and run 10 realizations apiece \linebreak ({\tt num\_realizations/num\_groups}), one after another. Thus, 100 realizations are executed simultaneously with each processor group simulating a single realization on 100 processor cores at a time. Each processor group continues to run realizations until its allocation of 10 is completed.

\noindent
To simulate a specific realization without running in multi-realization stochastic mode use:

{\tt mpirun -np 10000 pflotran -realization\_id <integer>}

\noindent
where <integer> specifies the realization id.

\begin{comment}
\subsection{Building SAMRAI Version 2.4.4}

\noindent
To build SAMRAI follow the instructions listed below.

{\tt mkdir SAMRAI-v2.4.4}

{\tt cd SAMRAI-v2.4.4}

{\tt hg clone http://software.lanl.gov/pflotran/hg/samrai SAMRAI}

{\tt mkdir samrai-objs in directory SAMRAI-v2.4.4}

{\tt cd samrai-objs}

Define the environment variables $\verb|SAMRAI_INSTALL_DIR|$, $\verb|MPI_HOME|$, $\verb|HDF5_HOME|$, \linebreak $\verb|PETSC_DIR|$ to be the top level directories of the appropriate packages

\noindent
With MPICH:
\begin{verbatim}
../SAMRAI/configure --prefix=${SAMRAI_INSTALL_DIR} \
--with-CC=mpicc \
--with-CXX=mpicxx \
--with-F77=mpif90 \
--with-mpi \
--with-mpi-include=${MPI_HOME}/include \
--with-mpi-lib-dirs=${MPI_HOME}/lib \
--with-MPICC=mpicc \
--with-x \
--with-hdf5=${HDF5_HOME} \
--with-hypre=${HYPRE_HOME} \
--with-petsc=${PETSC_DIR} \
--with-blaslapack \
--enable-opt \
 --enable-debug \
--enable-char \
--enable-bool \
CXXFLAGS="-DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX" \
CPPFLAGS="-DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX"
\end{verbatim}

\noindent
With OPENMPI:
\begin{verbatim}
../${SAMRAI_SRC_DIR}/configure \
--prefix=${SAMRAI_INSTALL_DIR} \
--with-CC=mpicc \
--with-CXX=mpicxx \
--with-F77=mpif90 \
--with-mpi \
--with-mpi-include=${MPI_HOME}/include 
--with-mpi-lib-dirs=${MPI_HOME}/lib 
--with-MPICC=mpicc \
--with-x \
--with-hdf5=${HDF5_INSTALL_DIR} \
--with-hypre=${HYPRE_INSTALL_DIR} \
--with-petsc=${PETSC_DIR} \
--with-blaslapack \
--disable-opt \
--enable-debug \
--enable-char \
--enable-bool \
CXXFLAGS="-DOMPI_IGNORE_CXX_SEEK -DOMPI_SKIP_MPICXX \
  -I${PETSC_DIR}/${PETSC_ARCH}/include -I${PETSC_DIR}/include" \
CPPFLAGS="-DOMPI_IGNORE_CXX_SEEK -DOMPI_SKIP_MPICXX"
\end{verbatim}

{\tt make}

{\tt make install}

Define the environment variable $\verb|SAMRAI|$ to point to $\verb|SAMRAI_INSTALL_DIR|$ (for example in your .cshrc)

Instructions for SAMR utils package

{\tt hg clone  http://software.lanl.gov/pflotran/hg/samrutils}

{\tt cd samrutils}

Define the environment variable $\verb|AMRUTILITIES_HOME|$ to be the directory where you want the libraries and headers installed, it can point to the top level src dir if you like.

\verb|make prefix=${AMRUTILITIES_HOME} lib3d|

Instructions for SAMR solvers package

{\tt hg clone  http://software.lanl.gov/pflotran/hg/samrsolvers}

{\tt cd samrsolvers}

Define the environment variable $\verb|SAMRSOLVERS_HOME|$ to be the directory where you want the libraries and headers installed, it can point to the top level src dir if you like.

\verb|make prefix=${SAMRSOLVERS_HOME} lib3d|

\noindent
Assumptions:
\begin{enumerate}
\item External packages needed by PFLOTRAN are built: petsc-dev, hypre,  
hdf5, \linebreak mpich2/openmpi/some mpi
\item External packages needed by PFLOTRAN AMR interface are built:  
SAMRAI, SAMRUtils, SAMRSolvers
\end{enumerate}

\noindent
Define the environment variables:
\begin{center}
\begin{tabularx}{\linewidth}{lX}
\bf SAMRAI &--points to where SAMRAI is installed\\
\bf AMRUTILITIES\_HOME &--points to where SAMRUtils is installed\\
\bf SAMRSOLVERS\_HOME &--points to where SAMRSolvers is installed
\end{tabularx}
\end{center}

\noindent
Instructions for building $\verb|SAMRAIDriver|$:
\begin{center}
\begin{verbatim}
cd pflotran/src/pflotran
make samr_hdf5=1 pflotranamr

cd samr/src
make
\end{verbatim}
\end{center}

\noindent
At this point there should exist an executable named SAMRAIDriver.

\subsubsection{Running PFLOTRAN/SAMRAI on Jaguar}

Add to the {\tt .tcshrc} (or equivalent) file:
\begin{verbatim}
umask 0002
set LD_LIBRARY_PATH=/usr/lib64:${LD_LIBRARY_PATH}
setenv ARCH cray-xt5
setenv COMPILER gcc-4.4.2
setenv OPT opt
#setenv OPT debug 
setenv PROJ_DIR /tmp/proj/csc025/csc025geo3
setenv PKGS ${PROJ_DIR}/packages
setenv PKG_POSTFIX ${ARCH}-${COMPILER}-${OPT}
setenv PETSC_ARCH ${PKG_POSTFIX} 
setenv PETSC_VERSION petsc-dev
setenv PETSC_DIR ${PKGS}/petsc/${PETSC_VERSION}-${OPT}
setenv SAMRAI ${PKGS}/samrai/SAMRAI-v2.4.4/${PETSC_VERSION}-${PKG_POSTFIX}
setenv AMRUTILITIES_HOME ${PKGS}/samrutils/${PKG_POSTFIX}
setenv SAMRSOLVERS_HOME ${PKGS}/samrsolvers/${PKG_POSTFIX}
setenv PCH ${SAMRSOLVERS_HOME} 
 
setenv MPICH_UNEX_BUFFER_SIZE 400M
setenv MPICH_PTL_OTHER_EVENTS 4096
setenv MPICH_PTL_SEND_CREDITS -1
setenv MPICH_MSGS_PER_PROC 60000
\end{verbatim}

\noindent
Load the following modules:
\begin{verbatim}
module swap PrgEnv-pgi PrgEnv-gnu
 
module load mercurial
module load hdf5-parallel/1.8.3.1
module load hypre/2.4.0b
module load szip/2.1
module load totalview/8.6.0-1
\end{verbatim}

%########################################################################

\noindent
The user will need to set: PETSC\_DIR, PETSC\_ARCH, AMRUTILITIES\_HOME, SAMRSOLVERS\_HOME
to be defined as above. Also, note the swap to gnu.
\begin{verbatim}
 
cd pflotran
hg pull -u

cd src/pflotran
make hdf5=1 jaguar=1 pflotranamr

cd samr/src
make hdf5=1 jaguar=1
\end{verbatim}
At this point you should have an executable \verb|SAMRAIDriver|.
 
%\noindent
%To run the code do:

%\noindent
%\verb|cd nmg.4320|\\
%\verb|qsub run.4320|
 
%\noindent
%This should run the coupled flow (richards) and transport (15 species).

\end{comment}

\begin{comment}
The following instructions should aid in installing openmpi, PETSc, HDF5 and PFLOTRAN on a UNIX or Mac computer running MacOSX 10.4 or later.

\subsection{Openmpi}

Set environment variables \verb|PKGS| and \verb|MPI_HOME| and the appropriate \verb|PATH|:

\verb|setenv PKGS /Users/lichtner/petsc/packages|

\verb|setenv MPI_HOME $PKGS/openmpi/openmpi-1.2.5-gcc-4.0.1-absoft-10.1|

\verb|setenv PATH \$PKGS/openmpi/openmpi-1.2.5-gcc-4.0.1-absoft-10.1:\$PATH|

\verb|setenv F90 f90|

\verb|setenv F77 'f90 -YEXT_NAMES=LCS -YEXT_SFX= -f'|

\verb|setenv FC 'f90 -YEXT_NAMES=LCS -YEXT_SFX= -f'|

\verb|setenv CC gcc|

\noindent
Configure using:

\verb|./configure --prefix=$PKGS/openmpi/openmpi-1.2.6-gcc-4.0.1-absoft-10.1|

\noindent
Finally, compile, check installation and install:

\verb|make|

\verb|make check|

\verb|make install|

\subsection{PETSc}

PFLOTRAN uses the Developer version of PETSc. To install PETSc first set the enviroment variables \verb|PETSC_DIR| and \verb|PETSC_ARCH|:

\verb|setenv PETSC_DIR /Users/lichtner/petsc/petsc-dev|

\verb|setenv PETSC_ARCH Intel_MacOSX10.5|

\noindent
Configure PETSc on a Mac using openmpi and Fortran 90 Absoft 10.1:
\begin{verbatim}
./config/configure.py
--with-blas-lapack-lib="-framework vecLib"
--with-mpi-dir=$PKGS/openmpi/openmpi-1.2.6-gcc-4.0.1-absoft-10.1
--with-debugging=0
--with-shared=0
\end{verbatim}

\noindent
Compile and test the PESTc installation with:

\verb|make all test|

\noindent
Optionally install PETSc:

\verb|make install|


\subsection{HDF5}

To install HDF5 set the following environment variables:

\verb|setenv HDF5_INCLUDE $PKGS/hdf/hdf5-1.6.7-gcc-4.0.1-absoft-10.1/include|

\verb|setenv HDF5_LIB $PKGS/hdf/hdf5-1.6.7-gcc-4.0.1-absoft-10.1/lib|

\verb|setenv CC $MPI_HOME/bin/mpicc|

\verb|setenv F9X $MPI_HOME/bin/mpif90|

\verb|setenv CFLAGS -fno-strict-aliasing|

\verb|setenv FFLAGS ""|

\begin{verbatim}
./configure --enable-fortran
--prefix=$PKGS/hdf/hdf5-1.6.7-gcc-4.0.1-absoft-10.1
--disable-debug --enable-production --enable-parallel
--enable-static --disable-shared
\end{verbatim}

\verb|make|

\verb|make check|

\verb|make install|

\subsection{PFLOTRAN}

Compile PFLOTRAN using the command

\verb|make [hdf5=1] pflotran|

\noindent
Create input file \verb|pflotran.in| and run PFLOTRAN with the command:

\verb|mpirun -np #proc pflotran|

\noindent
where \verb|#proc| is the desired number of processor cores.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Direct Solvers}

To implement direct solvers in PETSc with PFLOTRAN first recompile PETSc with the options (petsc-dev, MacOSX 10.5):

\begin{verbatim}
./config/configure.py --with-blas-lapack-lib="-framework vecLib"
--with-mpi-dir=\$PKGS/openmpi/openmpi-1.2.7-gcc-4.0.1-absoft-10.1
--with-debugging=0 --with-shared=0
--download-mumps=1
--download-parmetis=1 --with-parmetis
--download-scalapack=1 --with-scalapack
--download-blacs=1 --with-blacs
\end{verbatim}

\noindent
Then run PFLOTRAN with the command-line options:
\begin{verbatim}
-flow\_mat\_type mpiaij
-flow\_ksp\_type preonly
-flow\_pc\_type lu
-flow\_pc\_factor\_mat\_solver\_package mumps
\end{verbatim}

\subsection{Trilinos}

The Trilinos package for multi-level solvers (ML) may be used with PETSc by configuring petsc-dev with the flag

\begin{verbatim}
./config/configure.py ... --download-ml
\end{verbatim}

When running PFLOTRAN add \verb|-flow_pc_type ml| and/or \verb|-trans_pc_type ml|.
ML options are given in the manual page for PCML (not tested).

\subsection{Condition Number of the Preconditioned Jacobian Matrix}

Since the preconditioned matrix is not explicitly computed, it is necessary to estimate the condition number of the preconditioned Jacobian matrix using PETSc during a PFLOTRAN run. To do this specify a KSP type of GMRES (the Hessenberg matrix is needed that is constructed as part of the Arnoldi process), and then specify

 {\tt -ksp\_monitor\_singular\_value}

The ratio of the largest to smallest singular values gives the condition number estimate for the preconditioned operator.  Note that this flag will also cause the 2-norm of the true residual (as opposed to the preconditioned residual) to be printed.

If you are doing this in the current version of PFLOTRAN, you need

 {\tt -flow\_ksp\_type gmres -flow\_ksp\_monitor\_singular\_value}

The estimates will get better the closer one is to the GMRES restart.  (When restart occurs, the Hessenberg matrix from which the eigenvalue estimates are obtained gets discarded along with everything else.)  The frequency can be changed via the option

 {\tt -flow\_ksp\_gmres\_restart <positive integer>}

The default restart frequency is 30.
\end{comment}
